# الفصل الرابع عشر: بناء منتجات AI متوافقة

في الفصل السابق، استعرضنا المشهد التنظيمي العام للذكاء الاصطناعي في المملكة العربية السعودية. الآن، ننتقل من "ماذا يقول النظام؟" إلى "كيف نطبق النظام؟". هذا الفصل هو دليلك العملي لتحويل السطور البرمجية إلى منتج ذكاء اصطناعي (AI) لا يتميز بالكفاءة التقنية فحسب، بل يمتثل بالكامل لمبادئ أخلاقيات الذكاء الاصطناعي الصادرة عن الهيئة السعودية للذكاء الاصطناعي (سدايا - SDAIA).

بصفتك رائد أعمال أو مطوراً في السوق السعودي، يجب أن تدرك أن الامتثال ليس "عقبة" في طريق الابتكار، بل هو "درع" يحمي شركتك من المساءلة القانونية ويبني جسر الثقة مع المستخدم المحلي والجهات الحكومية.

---

## 1. التحيز الخوارزمي (Algorithmic Bias) - كيف تتجنبه؟

التحيز في الذكاء الاصطناعي ليس مجرد خطأ تقني، بل هو مخاطرة قانونية واجتماعية. في السياق السعودي، قد يظهر التحيز في نماذج التوظيف، التمويل، أو حتى في تقديم الخدمات اللوجستية بناءً على المناطق الجغرافية.

### مبدأ العدالة (Fairness) في وثيقة "سدايا"
تنص مبادئ "سدايا" على ضرورة ضمان العدالة ومنع التمييز. التحيز الخوارزمي يحدث عندما تُنتج الخوارزمية نتائج غير عادلة تجاه فئة معينة بناءً على سمات محمية (مثل الجنس، العمر، المنطقة الجغرافية، أو الحالة الاجتماعية).

### خطوات عملية لتجنب التحيز:
1.  **تدقيق البيانات (Data Auditing):** قبل البدء بالتدريب، اسأل نفسك: هل تمثل البيانات كافة فئات المجتمع السعودي؟ إذا كنت تبني نموذجاً لتقييم الجدارة الائتمانية، هل بياناتك تقتصر على المدن الكبرى (الرياض، جدة) وتهمل المناطق الأخرى؟
2.  **اختيار المتغيرات (Feature Selection):** تجنب استخدام متغيرات قد تكون "وكيلة" (Proxies) للتحيز. على سبيل المثال، قد لا تستخدم "الجنس" مباشرة، لكنك تستخدم "تخصصاً جامعيًا" يغلب عليه جنس معين، مما يؤدي لنتائج متحيزة بشكل غير مباشر.
3.  **اختبارات ما بعد التدريب (Post-deployment Testing):** استخدم أدوات مثل (Fairlearn) أو (AI Fairness 360) لقياس الفجوة في النتائج بين المجموعات المختلفة.

> **نصيحة عملية:** قم بتعيين "مسؤول أخلاقيات" (Ethics Officer) ضمن فريق التطوير، وظيفته هي لعب دور "محامي الشيطان" واختبار النموذج بحثاً عن أي تحيزات محتملة قبل الإطلاق التجريبي.

---

## 2. قابلية التفسير (Explainability)

أكبر مخاوف المنظمين في السعودية هي "الصندوق الأسود" (Black Box). إذا رفض تطبيقك طلباً لقرض تمويلي، أو استبعد مرشحاً لوظيفة، فمن حق المستخدم -بموجب نظام حماية البيانات الشخصية (PDPL)- معرفة "لماذا".

### الفرق بين الشفافية وقابلية التفسير
*   **الشفافية (Transparency):** إخبار المستخدم بأنه يتعامل مع ذكاء اصطناعي.
*   **قابلية التفسير (Explainability):** القدرة على شرح المنطق الذي اتخذه النموذج للوصول لقرار معين بلغة يفهمها البشر.

### كيف تجعل منتجك قابلاً للتفسير؟
1.  **استخدام النماذج البيضاء (White-box Models):** في القرارات الحساسة (مثل الرعاية الصحية أو التمويل)، يفضل استخدام أشجار القرار (Decision Trees) أو النماذج الخطية التي يسهل تتبع منطقها.
2.  **تقنيات التفسير البعدي (Post-hoc Explainability):** إذا كنت مضطراً لاستخدام نماذج معقدة (Deep Learning)، استخدم أدوات مثل:
    *   **SHAP (SHapley Additive exPlanations):** لتوضيح وزن كل متغير في القرار النهائي.
    *   **LIME (Local Interpretable Model-agnostic Explanations):** لشرح قرارات فردية محددة.
3.  **واجهة المستخدم (UI/UX):** لا تعرض للمستخدم معادلات رياضية. بدلاً من ذلك، اعرض رسماً بيانياً بسيطاً يقول: "تم رفض الطلب لأن الدخل الشهري أقل من الحد المطلوب بنسبة 10%".

---

## 3. حوكمة نماذج الـ LLM (Large Language Models)

مع انفجار ثورة النماذج اللغوية الكبيرة، أصبح لزاماً على الشركات التقنية في السعودية وضع إطار حوكمة صارم، خاصة عند استخدام نماذج مثل GPT-4 أو "علام" (Allam) السعودي.

### التحديات والمخاطر في السوق السعودي:
*   **الهلوسة (Hallucinations):** تقديم معلومات خاطئة عن الأنظمة السعودية.
*   **تسرب البيانات (Data Leakage):** إرسال بيانات العملاء الحساسة إلى خوادم خارجية لتدريب النماذج.
*   **المحتوى غير الملائم:** إنتاج محتوى لا يتوافق مع القيم الثقافية والدينية للمملكة.

### إطار حوكمة الـ LLM المقترح:

| المجال | الإجراء المطلوب |
| :--- | :--- |
| **خصوصية البيانات** | استخدام تقنيات (RAG - Retrieval-Augmented Generation) لربط النموذج ببياناتك الخاصة دون إرسالها للتدريب الخارجي. |
| **هندسة الأوامر (Prompt Engineering)** | وضع "System Prompts" صارمة تمنع النموذج من الخروج عن السياق المهني أو الإجابة في مواضيع سياسية أو دينية حساسة. |
| **التدقيق البشري (HITL)** | تطبيق مبدأ "Human-in-the-Loop" للمراجعة البشرية قبل اعتماد المخرجات النهائية للنموذج في القرارات القانونية أو الطبية. |
| **توطين الاستضافة** | يفضل استخدام نماذج يمكن استضافتها محلياً (On-premise) أو عبر سحابة سعودية معتمدة لضمان عدم خروج البيانات خارج المملكة. |

---

## 4. استخدام البيانات السعودية في التدريب

البيانات هي الوقود، وفي السعودية، يخضع هذا الوقود لأنظمة صارمة تحت إشراف "سدايا" ومكتب إدارة البيانات الوطنية (NDMO).

### ضوابط استخدام البيانات المحلية:
1.  **السيادة الوطنية للبيانات:** بموجب نظام حماية البيانات الشخصية (PDPL)، تُصنف بعض البيانات كبيانات "حساسة". تدريب النماذج على بيانات الهوية الوطنية أو السجلات الصحية يتطلب موافقات خاصة وتشفيراً عالي المستوى.
2.  **الحصول على الموافقة (Consent):** لا يمكنك سحب بيانات المستخدمين من تطبيقك واستخدامها لتدريب نموذج جديد دون موافقة صريحة ومحددة لهذا الغرض (المادة 7 من نظام حماية البيانات الشخصية).
3.  **إخفاء الهوية (Anonymization):** قبل إدخال أي بيانات سعودية في دورة التدريب، يجب التأكد من إزالة كافة المعرفات الشخصية بطريقة لا يمكن عكسها.

### نصيحة للمطورين:
عند استخدام بيانات "مفتوحة" من بوابة البيانات الوطنية (open.data.gov.sa)، تأكد من قراءة رخصة الاستخدام. ليست كل البيانات المفتوحة مسموحة للاستخدام التجاري في تدريب نماذج الذكاء الاصطناعي دون قيود.

---

## 5. دراسة حالة: Chatbot حكومي متوافق

تخيل أنك فزت بعقد لتطوير "مساعد ذكي" لوزارة سعودية للإجابة على استفسارات المواطنين حول الأنظمة القانونية. كيف تبنيه ليكون متوافقاً؟

### المرحلة الأولى: التصميم (Privacy by Design)
*   **القرار:** لن نقوم بتخزين أرقام الهوية في ذاكرة المحادثة.
*   **التطبيق:** استخدام تقنية (PII Redaction) لحذف أي بيانات شخصية تظهر في الدردشة تلقائياً.

### المرحلة الثانية: التدريب والبيانات
*   **المصدر:** يتم تدريب النموذج فقط على الأنظمة واللوائح المنشورة في جريدة "أم القرى" والمنصات الرسمية.
*   **التوطين:** يتم استضافة النموذج على سحابة (Oracle) أو (Alibaba Cloud) داخل مراكز بيانات في الرياض لضمان الامتثال لسياسة سيادة البيانات.

### المرحلة الثالثة: الاختبار (Red Teaming)
*   **الاختبار:** محاولة استفزاز البوت لتقديم نصيحة قانونية خاطئة أو انتقاد جهة رسمية.
*   **النتيجة:** وضع قيود برمجية (Guardrails) تمنع البوت من الإجابة على أي سؤال خارج نطاق اختصاصه الوزاري.

### المرحلة الرابعة: التفسير والمسؤولية
*   **الواجهة:** إضافة تنبيه في أسفل المحادثة: "هذا الرد آلي لغرض الاسترشاد فقط، ولا يعتبر وثيقة قانونية رسمية".
*   **المسؤولية:** توفير خيار "التحدث مع موظف بشري" في حال لم يكن الرد مقنعاً للمستخدم.

---

## نصائح ذهبية لرواد الأعمال (Checklist)

قبل إطلاق منتج الـ AI الخاص بك في السوق السعودي، تأكد من استيفاء القائمة التالية:

*   [ ] هل قمت بإعداد "مذكرة تقييم الأثر على حماية البيانات" (DPIA)؟
*   [ ] هل نموذجك يلتزم بمبادئ "سدايا" السبعة (الرفاهية الإنسانية، الخصوصية، العدالة، الاعتمادية، الشفافية، المسؤولية، الأمان)؟
*   [ ] هل توجد آلية واضحة للمستخدمين للاعتراض على القرارات الآلية؟
*   [ ] هل بيانات التدريب خالية من التحيزات الجغرافية أو القبلية أو الجندرية؟
*   [ ] هل قمت بتوثيق "سجل الأنشطة" لمعالجة البيانات (RoPA)؟

---

## تحذيرات من أخطاء شائعة

1.  **الاعتماد الكلي على API خارجي:** استخدام (OpenAI API) مباشرة دون طبقة حماية (Middleware) لفلترة البيانات الحساسة قد يعرضك لمخالفة نظام حماية البيانات الشخصية.
2.  **إهمال الثقافة المحلية:** تدريب نموذج على بيانات غربية فقط قد يجعله يقدم إجابات لا تتناسب مع العادات والتقاليد السعودية، مما يؤدي لنفور المستخدمين وفشل المنتج تجارياً.
3.  **ادعاء الدقة المطلقة:** تجنب التسويق لمنتجك بأنه "لا يخطئ". في الأنظمة السعودية، يجب أن تكون صريحاً بشأن حدود قدرات الذكاء الاصطناعي.

> **خاتمة الفصل:**
> إن بناء منتج ذكاء اصطناعي متوافق في المملكة العربية السعودية ليس مجرد التزام قانوني، بل هو استثمار في استدامة شركتك. الجهات التنظيمية مثل "سدايا" ترحب بالابتكار، لكنها تضع "الإنسان" دائماً في قلب المعادلة. تذكر دائماً: **الخوارزمية الذكية هي الخوارزمية التي تحترم مستخدمها ونظامه.**

في الفصل القادم، سنتناول موضوعاً لا يقل أهمية: **الملكية الفكرية في العصر الرقمي - كيف تحمي كودك وعلامتك التجارية؟**